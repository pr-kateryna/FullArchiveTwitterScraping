import requests
import os
import json
import csv
import dateutil.parser

# Input your token generated by Twiiter Developer Lisence Account
os.environ['TOKEN'] = ''

def auth():
    return os.getenv('TOKEN')

def create_headers(bearer_token):
    headers = {"Authorization": "Bearer {}".format(bearer_token)}
    return headers

# For more deteails on available parameters see
# https://developer.twitter.com/en/docs/twitter-api/tweets/search/api-reference/get-tweets-search-all
def create_url(keyword, start_date, end_date, max_result = 500):
    search_url = "https://api.twitter.com/2/tweets/search/all"
    query_params = {
        'query': keyword,
        'start_time': start_date,
        'end_time': end_date,
        'max_results': max_result,
        'expansions': 'author_id,in_reply_to_user_id,referenced_tweets.id,geo.place_id',
        'tweet.fields': 'id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source',
        'user.fields': 'id,name,username,created_at,description,public_metrics,verified',
        'place.fields': 'full_name,id,country,country_code,geo,name,place_type',
        'next_token': {}
    }
    return(search_url, query_params)

def connect_to_endpoint(url, headers, params, next_token = None):
    params['next_token'] = next_token
    response = requests.request("GET", url, headers = headers, params = params)
    print("Endpoint Response Code: " + str(response.status_code))
    if response.status_code != 200:
        raise Exception(response.status_code, response.text)
    return response.json()

def append_to_csv(json_response, fileName):
    
    counter = 0
    csvFile = open(fileName, "a", newline="", encoding='utf-8')
    csvWriter = csv.writer(csvFile)

    for tweet in json_response['data']:
        author_id = tweet['author_id']
        created_at = dateutil.parser.parse(tweet['created_at'])
        tweet_id = tweet['id']
        text = tweet['text']
        lang = tweet['lang']
        convo_id = tweet['conversation_id']
        retweet_count = tweet['public_metrics']['retweet_count']
        reply_count = tweet['public_metrics']['reply_count']
        like_count = tweet['public_metrics']['like_count']
        quote_count = tweet['public_metrics']['quote_count']
        user = ""
        username = ""
        account_creation = ""
        for one_user in json_response['includes']['users']:
          if author_id == one_user['id']:
            user = one_user['name']
            username = one_user['username']
            description = one_user['description']
            account_creation = one_user ['created_at']
            break
        res = [author_id, user, username, account_creation, description, created_at, tweet_id, text, lang, convo_id, retweet_count, reply_count, like_count, quote_count]
        csvWriter.writerow(res)
        counter += 1
    csvFile.close()
    print("# of Tweets added from this response: ", counter)


# Customized request
bearer_token = auth()
headers = create_headers(bearer_token)

# Fill your keyword.
# Visa, Application
keyword = ''

# Choose time scope for scraping.
# Ex. 2020-12-25T00:00:00Z
start_time = ''
end_time = ''
max_results = 500


url = create_url(keyword, start_time, end_time, max_results)
json_response = connect_to_endpoint(url[0], headers, url[1])

append_to_csv(json_response, "KeywordFull.csv")
